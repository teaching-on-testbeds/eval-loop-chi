{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (Optional) Use in a continuous monitoring and re-training pipeline\n",
    "\n",
    "Finally, let’s take a quick look at how we would finish “closing the loop” by using the labeled data in a continuous monitoring and re-training pipeline. We will want to automate the process of:\n",
    "\n",
    "-   creating tasks in Label Studio out of data from production\n",
    "-   evaluating performance on labeled data sampled from production\n",
    "-   and re-training on data from production\n",
    "\n",
    "There are a variety of ways in which we can realize this goal. We will use Apache Airflow, a workflow orchestrator, to manage this pipeline on a schedule. (Airflow is a good fit for a Docker environment; in a Kubernetes environment, we might prefer to use Argo Events + Argo Workflow.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let’s bring up Airflow:\n",
    "\n",
    "``` bash\n",
    "# runs on node-eval-loop\n",
    "docker compose -f eval-loop-chi/docker/docker-compose-airflow.yaml up -d\n",
    "```\n",
    "\n",
    "When it comes up, a web UI will be on port 8081. (Airflow runs a web server on port 8080 by default, but since we already have Label Studio on port 8080, we used the Docker compose to map it to port 8081 instead.)\n",
    "\n",
    "In a browser, open\n",
    "\n",
    "    http://A.B.C.D:8081\n",
    "\n",
    "substituting the floating IP assigned to your instance in place of `A.B.C.D`. Log in with username `airflow@example.com` and password `airflow` (we have created an initial user with these credentials in our Docker compose file)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Airflow is a workflow orchestrator for running any pipeline that represented as a DAG - directed acyclic graph. Here’s an example of basic DAG for Airflow:\n",
    "\n",
    "``` python\n",
    "from airflow import DAG\n",
    "from airflow.operators.empty import EmptyOperator\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "with DAG(\n",
    "    dag_id=\"test_dag_hello_world\",\n",
    "    start_date=datetime.today() - timedelta(days=1),\n",
    "    schedule_interval=\"@daily\",\n",
    "    catchup=False,\n",
    ") as dag:\n",
    "    t1 = EmptyOperator(task_id=\"start\")\n",
    "```\n",
    "\n",
    "This Python script defines a DAG called “test_dag_hello_world”\n",
    "\n",
    "-   that starts one day ago (e.g. it is allowed to run as of one day ago; it was not allowed to run before that date)\n",
    "-   that is scheduled to run daily\n",
    "-   and doesn’t “catch up” on past runs, e.g. if I set the start date to one year ago, it wouldn’t run 365 times to make up for the missing runs!)\n",
    "\n",
    "The actual DAG just has one “node”, `t1`, and that runs Airflow’s built-in `EmptyOperator` which acts as a no-op placeholder task.\n",
    "\n",
    "When we place a Python file in the Airflow DAGs folder, Airflow scans that file at regular intervals to discover DAG definitions. If a file has top-level variables that are instances of the DAG class, Airflow adds it to its metadata database and displays it in the web UI.\n",
    "\n",
    "In the Airflow web UI, the `test_dag_hello_world` DAG should be visible in DAGs tab. It runs on a schedule, but we can also trigger it manually - press the ▶ button to trigger it now. Confirm that it runs sucessfully."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let’s run a “real” pipeline. Our first pipeline will:\n",
    "\n",
    "-   Get objects from the “production” bucket that have been uploaded in the intervening interval since the last DAG run.\n",
    "-   Sample from them to get tasks to send to Label Studio, including: a random sample of all images, the low confidence images, the flagged images, and images that have been re-labeled by the user.\n",
    "-   Move these to a “production-label-wait” bucket, then generate tasks for Label Studio.\n",
    "-   and, move the remaining (not selected for labeling) high=confidence images from the list to a “production-noisy” bucket.\n",
    "\n",
    "The “production-noisy” bucket will be used for model re-training. We consider it “noisy” because its labels are generated by the model itself, not by human; but we will add human-labeled data to it when available, in the next pipeline.\n",
    "\n",
    "This DAG will take advantage of Airflow’s built in “data interval” idea, which lets the DAG know what time interval of data it is responsible for processing according to its schedule:\n",
    "\n",
    "    start = context['data_interval_start']\n",
    "    end = context['data_interval_end']\n",
    "\n",
    "although if we trigger it manually from the web UI, that won’t apply, so then we would just use a recent half-hour window.\n",
    "\n",
    "The DAG will also include a task that initializes the “production-label-wait” and “production-noisy” buckets if they do not yet exist, and it will create a Label Studio project for “Food11 Continuous X” if it does not yet exist.\n",
    "\n",
    "Click through to this DAG, and look at it in both Code view and Graph view.\n",
    "\n",
    "Then, upload 10-20 images to the Flask application. For at least a few images, correct the class label. Make sure you have uploaded a few images for which the model is known to have low confidence.\n",
    "\n",
    "Trigger the DAG manually in the web interface. Observe the effect in MinIO and in Label Studio."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In Label Studio, label some of the images in the “Food11 Continuous X” project. Then, we can run the next stage, which:\n",
    "\n",
    "-   Gets the new labels from Label Studio\n",
    "-   Copy the newly labeled images to “production-clean” and “production-noisy” buckets, and remove them from “production-label-wait”\n",
    "-   Compute the accuracy on the batch of data\n",
    "\n",
    "Click through to this second stage DAG, and look at it in both Code view and Graph view.\n",
    "\n",
    "Trigger the DAG manually in the web interface. Observe the effect in MinIO."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Airflow is an extremely capable platform, and we have barely scratched the surface of what we can do with it - but now, we have the basic pieces of a continuous monitoring and re-training pipeline in place!\n",
    "\n",
    "We have a “production-clean” bucket suitable for evaluation (with reliable labels generated by our human annotator) and a “production-noisy” bucket suitable for re-training, with labels that many not be accurate (since most are labeled by our own model!)\n",
    "\n",
    "We could extend this pipeline to -\n",
    "\n",
    "-   push the batch accuracy to Prometheus, for continuous monitoring\n",
    "-   trigger re-training\n",
    "\n",
    "but we’ll stop here for now, since we have not set up our training and monitoring infrastructure in this experiment."
   ]
  }
 ],
 "nbformat": 4,
 "nbformat_minor": 4,
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3 (ipykernel)",
   "language": "python"
  },
  "language_info": {
   "name": "python",
   "codemirror_mode": {
    "name": "ipython",
    "version": "3"
   },
   "file_extension": ".py",
   "mimetype": "text/x-python"
  }
 }
}
