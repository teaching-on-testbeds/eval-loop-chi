{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate accuracy on production data\n",
    "\n",
    "We are going to:\n",
    "\n",
    "-   connect to Label Studio and retrieve the details of all the “tasks” associated with our Food11 project\n",
    "-   connect to MinIO, and get the predicted class (from the tag!) of every object in the “production” bucket\n",
    "\n",
    "and compare those, to evaluate the accuracy of our system on “production” data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# runs inside Jupyter container on node-eval-loop\n",
    "import requests\n",
    "import boto3 \n",
    "from urllib.parse import urlparse\n",
    "from collections import defaultdict, Counter\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we need to get the details we will need to authenticate to MinIO and to Label Studio. We passed these as environment variables to the Jupyter container:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# runs inside Jupyter container on node-eval-loop\n",
    "LABEL_STUDIO_URL = os.environ['LABEL_STUDIO_URL']\n",
    "LABEL_STUDIO_TOKEN = os.environ['LABEL_STUDIO_USER_TOKEN']\n",
    "PROJECT_ID = 1  # use the first project set up in Label Studio\n",
    "\n",
    "MINIO_URL = os.environ['MINIO_URL']\n",
    "MINIO_ACCESS_KEY = os.environ['MINIO_USER']\n",
    "MINIO_SECRET_KEY = os.environ['MINIO_PASSWORD']\n",
    "BUCKET_NAME = \"production\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can authenticate to MinIO:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# runs inside Jupyter container on node-eval-loop\n",
    "s3 = boto3.client(\n",
    "    \"s3\",\n",
    "    endpoint_url=MINIO_URL,\n",
    "    aws_access_key_id=MINIO_ACCESS_KEY,\n",
    "    aws_secret_access_key=MINIO_SECRET_KEY,\n",
    "    region_name='us-east-1'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And, we can authenticate to LabelStudio and get the details of all the “tasks”. (Each image that requires human annotation is one task.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# runs inside Jupyter container on node-eval-loop\n",
    "response = requests.get(\n",
    "    f\"{LABEL_STUDIO_URL}/api/projects/{PROJECT_ID}/export?exportType=JSON\",\n",
    "    headers={\"Authorization\": f\"Token {LABEL_STUDIO_TOKEN}\"}\n",
    ")\n",
    "\n",
    "tasks = response.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# runs inside Jupyter container on node-eval-loop\n",
    "tasks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can compute the accuracy of our model on the production data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# runs inside Jupyter container on node-eval-loop\n",
    "total, correct = 0, 0\n",
    "\n",
    "for task in tasks:\n",
    "    # get human annotator's from Label Studio\n",
    "    human_label = task['annotations'][0]['result'][0]['value']['choices'][0]\n",
    "    key = urlparse(task['data']['image']).path.lstrip(\"/\")\n",
    "    key = key[len(f\"{BUCKET_NAME}/\"):] if key.startswith(f\"{BUCKET_NAME}/\") else key\n",
    "\n",
    "    # get label assigned by model to that SAME IMAGE, from the object tag in MinIO\n",
    "    tags = s3.get_object_tagging(Bucket=BUCKET_NAME, Key=key)['TagSet']\n",
    "    model_label = {t['Key']: t['Value'] for t in tags}.get('predicted_class')\n",
    "\n",
    "    if model_label and human_label:\n",
    "        total += 1\n",
    "        correct += int(model_label == human_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# runs inside Jupyter container on node-eval-loop\n",
    "print(f\"Accuracy: {correct}/{total} = {correct / total:.2%}\" if total else \"No valid comparisons made.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example, we have computed simple accuracy over a static set of production images, as a demo. However, this could be integrated into a broader evaluation plan (including e.g. evaluation on different metrics, on specific slices of interest) and a broader monitoring plan (e.g. use the timestamp tag to monitor prediction accuracy over time using time windows.)\n",
    "\n",
    "Similarly, our labeled production data can be used as part of a continuous training plan - after evaluating our model on the labeled production data, we can use it as part of the training set the next time we re-train our model."
   ]
  }
 ],
 "nbformat": 4,
 "nbformat_minor": 4,
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3 (ipykernel)",
   "language": "python"
  },
  "language_info": {
   "name": "python",
   "codemirror_mode": {
    "name": "ipython",
    "version": "3"
   },
   "file_extension": ".py",
   "mimetype": "text/x-python"
  }
 }
}
